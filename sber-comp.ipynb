{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29296,"databundleVersionId":2344227,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom itertools import product\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import RandomizedSearchCV\nimport random\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:14:40.547674Z","iopub.execute_input":"2024-12-05T17:14:40.548063Z","iopub.status.idle":"2024-12-05T17:14:42.288484Z","shell.execute_reply.started":"2024-12-05T17:14:40.548030Z","shell.execute_reply":"2024-12-05T17:14:42.287386Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/sbermarket-internship-competition/sample_submission.csv\n/kaggle/input/sbermarket-internship-competition/train.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/sbermarket-internship-competition/train.csv')\ntest = pd.read_csv('/kaggle/input/sbermarket-internship-competition/sample_submission.csv')\ntrain['order_completed_at'] = pd.to_datetime(train['order_completed_at'])\ntrain.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:14:43.916921Z","iopub.execute_input":"2024-12-05T17:14:43.917569Z","iopub.status.idle":"2024-12-05T17:14:47.248392Z","shell.execute_reply.started":"2024-12-05T17:14:43.917525Z","shell.execute_reply":"2024-12-05T17:14:47.247294Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   user_id  order_completed_at  cart\n0        2 2015-03-22 09:25:46   399\n1        2 2015-03-22 09:25:46    14\n2        2 2015-03-22 09:25:46   198\n3        2 2015-03-22 09:25:46    88\n4        2 2015-03-22 09:25:46   157","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>order_completed_at</th>\n      <th>cart</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>2015-03-22 09:25:46</td>\n      <td>399</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>2015-03-22 09:25:46</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2015-03-22 09:25:46</td>\n      <td>198</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>2015-03-22 09:25:46</td>\n      <td>88</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>2015-03-22 09:25:46</td>\n      <td>157</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"**EDA**","metadata":{}},{"cell_type":"code","source":"train.describe(include='all')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:14:48.529627Z","iopub.execute_input":"2024-12-05T17:14:48.530107Z","iopub.status.idle":"2024-12-05T17:14:48.828722Z","shell.execute_reply.started":"2024-12-05T17:14:48.530066Z","shell.execute_reply":"2024-12-05T17:14:48.827621Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"            user_id             order_completed_at          cart\ncount  3.123064e+06                        3123064  3.123064e+06\nmean   7.253373e+03  2020-04-09 01:17:00.182836992  2.273235e+02\nmin    0.000000e+00            2015-03-22 09:25:46  0.000000e+00\n25%    2.884000e+03            2020-02-03 06:03:43  4.200000e+01\n50%    6.055000e+03            2020-05-19 06:35:20  1.460000e+02\n75%    1.117200e+04            2020-07-14 04:50:22  3.990000e+02\nmax    1.999900e+04            2020-09-03 23:45:45  8.800000e+02\nstd    5.337838e+03                            NaN  2.112867e+02","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>order_completed_at</th>\n      <th>cart</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>3.123064e+06</td>\n      <td>3123064</td>\n      <td>3.123064e+06</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>7.253373e+03</td>\n      <td>2020-04-09 01:17:00.182836992</td>\n      <td>2.273235e+02</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000e+00</td>\n      <td>2015-03-22 09:25:46</td>\n      <td>0.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>2.884000e+03</td>\n      <td>2020-02-03 06:03:43</td>\n      <td>4.200000e+01</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>6.055000e+03</td>\n      <td>2020-05-19 06:35:20</td>\n      <td>1.460000e+02</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.117200e+04</td>\n      <td>2020-07-14 04:50:22</td>\n      <td>3.990000e+02</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.999900e+04</td>\n      <td>2020-09-03 23:45:45</td>\n      <td>8.800000e+02</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>5.337838e+03</td>\n      <td>NaN</td>\n      <td>2.112867e+02</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"print(train.shape)\nprint(train.isna().sum().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:14:56.282778Z","iopub.execute_input":"2024-12-05T17:14:56.283344Z","iopub.status.idle":"2024-12-05T17:14:56.303171Z","shell.execute_reply.started":"2024-12-05T17:14:56.283302Z","shell.execute_reply":"2024-12-05T17:14:56.301893Z"}},"outputs":[{"name":"stdout","text":"(3123064, 3)\n0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"train['user_id'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:14:59.987191Z","iopub.execute_input":"2024-12-05T17:14:59.988374Z","iopub.status.idle":"2024-12-05T17:15:00.038300Z","shell.execute_reply.started":"2024-12-05T17:14:59.988322Z","shell.execute_reply":"2024-12-05T17:15:00.036879Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"user_id\n380      3508\n105      2833\n84       2610\n3918     2566\n1156     2357\n         ... \n9614        3\n8990        3\n3844        3\n19825       3\n19841       3\nName: count, Length: 20000, dtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"categs = train['cart'].value_counts()\ncategs.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:15:04.067996Z","iopub.execute_input":"2024-12-05T17:15:04.068994Z","iopub.status.idle":"2024-12-05T17:15:04.103261Z","shell.execute_reply.started":"2024-12-05T17:15:04.068941Z","shell.execute_reply":"2024-12-05T17:15:04.102002Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"cart\n57     108877\n14      93957\n61      91543\n398     81694\n23      71837\nName: count, dtype: int64"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\norder_counts = train['user_id'].value_counts()\nplt.figure(figsize=(10, 6))\nplt.hist(order_counts, bins=50)\nplt.xlabel('Количество заказов на пользователя')\nplt.ylabel('Частота')\nplt.title('Распределение количества заказов по пользователям')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T17:38:41.021798Z","iopub.execute_input":"2024-11-23T17:38:41.022310Z","iopub.status.idle":"2024-11-23T17:38:41.339116Z","shell.execute_reply.started":"2024-11-23T17:38:41.022260Z","shell.execute_reply":"2024-11-23T17:38:41.337935Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x600 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":29},{"cell_type":"markdown","source":"Добавление искуственных данных","metadata":{}},{"cell_type":"code","source":"train = train.sort_values(by=['user_id', 'order_completed_at'])\n\ntrain['target'] = 1\n\nnew_rows = []\n\nfor user_id, user_orders in train.groupby('user_id'):\n    \n    user_orders_by_date = user_orders.groupby('order_completed_at')\n    \n    all_previous_items = set()\n    \n    for order_date, order_group in user_orders_by_date:\n        current_order_items = set(order_group['cart'])\n        \n        not_ordered_items = all_previous_items - current_order_items\n\n        num_items_to_add = min(len(not_ordered_items), len(current_order_items))\n\n        items_to_add = random.sample(not_ordered_items, num_items_to_add)\n\n        for item in items_to_add:\n            new_row = {\n                'user_id': user_id,\n                'cart': item,  \n                'order_completed_at': order_date,\n                'target': 0,\n            }\n            new_rows.append(new_row)\n\n        all_previous_items.update(current_order_items)\n\n\nnew_data = pd.DataFrame(new_rows)\n\ntrain_with_zeros = pd.concat([train, new_data], ignore_index=True)\n\nprint(train_with_zeros.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:15:11.320684Z","iopub.execute_input":"2024-12-05T17:15:11.321729Z","iopub.status.idle":"2024-12-05T17:15:55.895812Z","shell.execute_reply.started":"2024-12-05T17:15:11.321679Z","shell.execute_reply":"2024-12-05T17:15:55.894661Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/74258621.py:20: DeprecationWarning: Sampling from a set deprecated\nsince Python 3.9 and will be removed in a subsequent version.\n  items_to_add = random.sample(not_ordered_items, num_items_to_add)\n","output_type":"stream"},{"name":"stdout","text":"   user_id  order_completed_at  cart  target\n0        0 2020-07-19 09:59:17    20       1\n1        0 2020-07-19 09:59:17    82       1\n2        0 2020-07-19 09:59:17   441       1\n3        0 2020-07-19 09:59:17    57       1\n4        0 2020-07-19 09:59:17    14       1\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"print(train_with_zeros.shape)\nprint(train_with_zeros.isna().sum().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:34:11.935292Z","iopub.execute_input":"2024-12-05T15:34:11.935660Z","iopub.status.idle":"2024-12-05T15:34:11.964332Z","shell.execute_reply.started":"2024-12-05T15:34:11.935628Z","shell.execute_reply":"2024-12-05T15:34:11.963309Z"}},"outputs":[{"name":"stdout","text":"(5708975, 4)\n0\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"**Train Validation Split**","metadata":{}},{"cell_type":"code","source":"cutoff_date = '2020-07-14'\n\n# Разделим на тренировочную и валидационную выборки, используя дату третьего квартиля \ntrain_set = train_with_zeros[train_with_zeros['order_completed_at'] < cutoff_date].copy()\nval_set = train_with_zeros[train_with_zeros['order_completed_at'] >= cutoff_date].copy()\nprint(f\"Количество записей в train_set: {len(train_set)}\")\nprint(f\"Количество записей в val_set: {len(val_set)}\")\nprint(f\"Количество записей в test: {len(test)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:16:01.199876Z","iopub.execute_input":"2024-12-05T17:16:01.200296Z","iopub.status.idle":"2024-12-05T17:16:01.813799Z","shell.execute_reply.started":"2024-12-05T17:16:01.200261Z","shell.execute_reply":"2024-12-05T17:16:01.812767Z"}},"outputs":[{"name":"stdout","text":"Количество записей в train_set: 4214901\nКоличество записей в val_set: 1494074\nКоличество записей в test: 790449\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"**Feature Engineering**","metadata":{}},{"cell_type":"code","source":"test[['user_id', 'cart']] = test['id'].str.split(';', expand=True)\ntest['user_id'] = test['user_id'].astype(int)\n\nprint('Пользователей в train_set: ', train_set['user_id'].nunique())\nprint('Пользователей в val_set: ', val_set['user_id'].nunique())\nprint('Пользователей в test: ', test['user_id'].nunique())\n\n\nval_users_not_in_train = set(val_set['user_id']) - set(train_set['user_id'])\nprint(f\"Количество пользователей в val_set, но отсутствующих в train_set: {len(val_users_not_in_train)}\")\n\ntest_users_not_in_train = set(test['user_id']) - set(train_set['user_id'])\nprint(f\"Количество пользователей в test, но отсутствующих в train_set: {len(test_users_not_in_train)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:34:29.857721Z","iopub.execute_input":"2024-12-05T15:34:29.858088Z","iopub.status.idle":"2024-12-05T15:34:32.758926Z","shell.execute_reply.started":"2024-12-05T15:34:29.858056Z","shell.execute_reply":"2024-12-05T15:34:32.757873Z"}},"outputs":[{"name":"stdout","text":"Пользователей в train_set:  18161\nПользователей в val_set:  16160\nПользователей в test:  13036\nКоличество пользователей в val_set, но отсутствующих в train_set: 1839\nКоличество пользователей в test, но отсутствующих в train_set: 1443\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Средннее количество позиций в заказах пользователя","metadata":{}},{"cell_type":"code","source":"order_sizes = train_set.groupby(['user_id', 'order_completed_at'])['cart'].nunique().reset_index(name='order_size')\navg_order_size = order_sizes.groupby('user_id')['order_size'].mean().reset_index(name='avg_order_size')\n\ntrain_set = train_set.merge(avg_order_size, on='user_id', how='left')\nval_set = val_set.merge(avg_order_size, on='user_id', how='left')\n\n# train_set.head()\nval_set.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:48:39.024622Z","iopub.execute_input":"2024-12-05T15:48:39.025025Z","iopub.status.idle":"2024-12-05T15:48:40.592133Z","shell.execute_reply.started":"2024-12-05T15:48:39.024992Z","shell.execute_reply":"2024-12-05T15:48:40.590934Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"   user_id  order_completed_at  cart  target  avg_order_size\n0        0 2020-07-19 09:59:17    20       1             NaN\n1        0 2020-07-19 09:59:17    82       1             NaN\n2        0 2020-07-19 09:59:17   441       1             NaN\n3        0 2020-07-19 09:59:17    57       1             NaN\n4        0 2020-07-19 09:59:17    14       1             NaN","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>order_completed_at</th>\n      <th>cart</th>\n      <th>target</th>\n      <th>avg_order_size</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2020-07-19 09:59:17</td>\n      <td>20</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2020-07-19 09:59:17</td>\n      <td>82</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2020-07-19 09:59:17</td>\n      <td>441</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>2020-07-19 09:59:17</td>\n      <td>57</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>2020-07-19 09:59:17</td>\n      <td>14</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"test = test.merge(avg_order_size, on='user_id', how='left')\ntest['avg_order_size'].isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:48:43.742530Z","iopub.execute_input":"2024-12-05T15:48:43.743395Z","iopub.status.idle":"2024-12-05T15:48:43.897509Z","shell.execute_reply.started":"2024-12-05T15:48:43.743353Z","shell.execute_reply":"2024-12-05T15:48:43.896448Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"53284"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"**Количество уникальных категорий, которые покупал каждый пользователь**","metadata":{}},{"cell_type":"code","source":"user_category_counts = train_set.groupby('user_id')['cart'].nunique().reset_index()\nuser_category_counts.columns = ['user_id', 'unique_category_count']\n\ntrain_set = train_set.merge(user_category_counts, on='user_id', how='left')\nval_set = val_set.merge(user_category_counts, on='user_id', how='left')\ntrain_set.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:48:59.533082Z","iopub.execute_input":"2024-12-05T15:48:59.533441Z","iopub.status.idle":"2024-12-05T15:49:00.324506Z","shell.execute_reply.started":"2024-12-05T15:48:59.533410Z","shell.execute_reply":"2024-12-05T15:49:00.323372Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"   user_id  order_completed_at  cart  target  avg_order_size  \\\n0        1 2019-05-08 16:09:41    55       1           5.875   \n1        1 2020-01-17 14:44:23    82       1           5.875   \n2        1 2020-01-17 14:44:23   798       1           5.875   \n3        1 2020-01-17 14:44:23    86       1           5.875   \n4        1 2020-01-17 14:44:23   421       1           5.875   \n\n   unique_category_count  \n0                     15  \n1                     15  \n2                     15  \n3                     15  \n4                     15  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>order_completed_at</th>\n      <th>cart</th>\n      <th>target</th>\n      <th>avg_order_size</th>\n      <th>unique_category_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2019-05-08 16:09:41</td>\n      <td>55</td>\n      <td>1</td>\n      <td>5.875</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2020-01-17 14:44:23</td>\n      <td>82</td>\n      <td>1</td>\n      <td>5.875</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2020-01-17 14:44:23</td>\n      <td>798</td>\n      <td>1</td>\n      <td>5.875</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2020-01-17 14:44:23</td>\n      <td>86</td>\n      <td>1</td>\n      <td>5.875</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2020-01-17 14:44:23</td>\n      <td>421</td>\n      <td>1</td>\n      <td>5.875</td>\n      <td>15</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"test = test.merge(user_category_counts, on='user_id', how='left')\ntest['unique_category_count'].isna().sum()\n# test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:58:42.731042Z","iopub.execute_input":"2024-11-24T09:58:42.731511Z","iopub.status.idle":"2024-11-24T09:58:42.826767Z","shell.execute_reply.started":"2024-11-24T09:58:42.731472Z","shell.execute_reply":"2024-11-24T09:58:42.825388Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"53284"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"Последняя дата покупки для каждой пары (user_id, category_id)","metadata":{}},{"cell_type":"code","source":"last_order_date = train_set.groupby(['user_id', 'cart'])['order_completed_at'].max().reset_index()\nlast_order_date.columns = ['user_id', 'cart', 'last_order_date']\n\ntrain_set = train_set.merge(last_order_date, on=['user_id', 'cart'], how='left')\nval_set = val_set.merge(last_order_date, on=['user_id', 'cart'], how='left')\ntrain_set.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:58:45.720705Z","iopub.execute_input":"2024-11-24T09:58:45.722033Z","iopub.status.idle":"2024-11-24T09:58:47.648294Z","shell.execute_reply.started":"2024-11-24T09:58:45.721983Z","shell.execute_reply":"2024-11-24T09:58:47.646999Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   user_id  order_completed_at  cart  target  avg_order_size  \\\n0        1 2019-05-08 16:09:41    55       1           5.875   \n1        1 2020-01-17 14:44:23    82       1           5.875   \n2        1 2020-01-17 14:44:23   798       1           5.875   \n3        1 2020-01-17 14:44:23    86       1           5.875   \n4        1 2020-01-17 14:44:23   421       1           5.875   \n\n   unique_category_count     last_order_date  \n0                     15 2020-05-24 11:13:59  \n1                     15 2020-05-24 11:13:59  \n2                     15 2020-05-24 11:13:59  \n3                     15 2020-04-14 01:31:20  \n4                     15 2020-03-29 13:26:49  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>order_completed_at</th>\n      <th>cart</th>\n      <th>target</th>\n      <th>avg_order_size</th>\n      <th>unique_category_count</th>\n      <th>last_order_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2019-05-08 16:09:41</td>\n      <td>55</td>\n      <td>1</td>\n      <td>5.875</td>\n      <td>15</td>\n      <td>2020-05-24 11:13:59</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2020-01-17 14:44:23</td>\n      <td>82</td>\n      <td>1</td>\n      <td>5.875</td>\n      <td>15</td>\n      <td>2020-05-24 11:13:59</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>2020-01-17 14:44:23</td>\n      <td>798</td>\n      <td>1</td>\n      <td>5.875</td>\n      <td>15</td>\n      <td>2020-05-24 11:13:59</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2020-01-17 14:44:23</td>\n      <td>86</td>\n      <td>1</td>\n      <td>5.875</td>\n      <td>15</td>\n      <td>2020-04-14 01:31:20</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2020-01-17 14:44:23</td>\n      <td>421</td>\n      <td>1</td>\n      <td>5.875</td>\n      <td>15</td>\n      <td>2020-03-29 13:26:49</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"last_order_date['cart'] = last_order_date['cart'].astype(str)\ntest['cart'] = test['cart'].astype(str)\ntest = test.merge(last_order_date, on=['user_id', 'cart'], how='left')\ntest['last_order_date'].isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:58:51.524226Z","iopub.execute_input":"2024-11-24T09:58:51.524692Z","iopub.status.idle":"2024-11-24T09:58:52.451412Z","shell.execute_reply.started":"2024-11-24T09:58:51.524653Z","shell.execute_reply":"2024-11-24T09:58:52.450330Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"179800"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# Заполнение пустых значений\n\n\ndefault_values = {\n    'avg_order_size': 0,              \n    'unique_category_count': 0,       \n    'last_order_date': pd.Timestamp('1900-01-01')  # Условная дата для отсутствия заказов у новых пользователей\n}\n\n\nval_set.fillna(default_values, inplace=True)\ntest.fillna(default_values, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:58:56.758352Z","iopub.execute_input":"2024-11-24T09:58:56.758843Z","iopub.status.idle":"2024-11-24T09:58:56.832226Z","shell.execute_reply.started":"2024-11-24T09:58:56.758801Z","shell.execute_reply":"2024-11-24T09:58:56.830866Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_set['target'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:59:00.391077Z","iopub.execute_input":"2024-11-24T09:59:00.391556Z","iopub.status.idle":"2024-11-24T09:59:00.427540Z","shell.execute_reply.started":"2024-11-24T09:59:00.391516Z","shell.execute_reply":"2024-11-24T09:59:00.426307Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"target\n1    2341288\n0    1873613\nName: count, dtype: int64"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"# train_set['target'] = 1\n\nX_train = train_set.drop(columns=['target', 'order_completed_at'])\ny_train = train_set['target']\nX_val = val_set.drop(columns=['target', 'order_completed_at'])\ny_val = val_set['target']\n\nprint(X_train.head())\nprint(X_val.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:59:03.992211Z","iopub.execute_input":"2024-11-24T09:59:03.992624Z","iopub.status.idle":"2024-11-24T09:59:04.118575Z","shell.execute_reply.started":"2024-11-24T09:59:03.992592Z","shell.execute_reply":"2024-11-24T09:59:04.117120Z"}},"outputs":[{"name":"stdout","text":"   user_id  cart  avg_order_size  unique_category_count     last_order_date\n0        1    55           5.875                     15 2020-05-24 11:13:59\n1        1    82           5.875                     15 2020-05-24 11:13:59\n2        1   798           5.875                     15 2020-05-24 11:13:59\n3        1    86           5.875                     15 2020-04-14 01:31:20\n4        1   421           5.875                     15 2020-03-29 13:26:49\n   user_id  cart  avg_order_size  unique_category_count last_order_date\n0        0    20             0.0                    0.0      1900-01-01\n1        0    82             0.0                    0.0      1900-01-01\n2        0   441             0.0                    0.0      1900-01-01\n3        0    57             0.0                    0.0      1900-01-01\n4        0    14             0.0                    0.0      1900-01-01\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"daily_orders = train.groupby(train_set['order_completed_at'].dt.date)['cart'].count()\n\n# Построение линейного графика\nplt.figure(figsize=(10, 5))\ndaily_orders.plot(kind='line')\nplt.title('Количество заказов по дням')\nplt.xlabel('Дата')\nplt.ylabel('Количество заказов')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:17:11.414870Z","iopub.execute_input":"2024-12-05T17:17:11.415662Z","iopub.status.idle":"2024-12-05T17:17:13.755136Z","shell.execute_reply.started":"2024-12-05T17:17:11.415619Z","shell.execute_reply":"2024-12-05T17:17:13.753971Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x500 with 1 Axes>","image/png":"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"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"Pipeline","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\nnumeric_features = ['avg_order_size', 'unique_category_count']\ncategorical_features = ['user_id', 'cart']\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())  \n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordinalencoder', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:59:12.602213Z","iopub.execute_input":"2024-11-24T09:59:12.602635Z","iopub.status.idle":"2024-11-24T09:59:12.610566Z","shell.execute_reply.started":"2024-11-24T09:59:12.602597Z","shell.execute_reply":"2024-11-24T09:59:12.609193Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import Pipeline\nimport lightgbm as lgb\nimport xgboost as xgb\n\n\nmodels = {\n    \"RandomForest\": RandomForestClassifier(random_state=42, class_weight='balanced'),\n    \"LightGBM\": lgb.LGBMClassifier(random_state=42, class_weight='balanced'),\n    \"XGBoost\": xgb.XGBClassifier(random_state=42, scale_pos_weight=y_train.value_counts()[0] / y_train.value_counts()[1])\n}\n\n\ndef evaluate_model(model, X_train, y_train, X_val, y_val, preprocessor):\n    \n    pipeline = Pipeline(steps=[\n        ('preprocessor', preprocessor),\n        ('classifier', model)\n    ])\n    \n    \n    pipeline.fit(X_train, y_train)\n    \n    \n    y_train_pred = pipeline.predict(X_train)\n    print(f\"Training Classification Report for {model.__class__.__name__}:\")\n    print(classification_report(y_train, y_train_pred))\n    \n    \n    y_val_pred = pipeline.predict(X_val)\n    print(f\"Validation Classification Report for {model.__class__.__name__}:\")\n    print(classification_report(y_val, y_val_pred))\n\n\nfor model_name, model in models.items():\n    print(f\"\\nEvaluating model: {model_name}\")\n    evaluate_model(model, X_train, y_train, X_val, y_val, preprocessor)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T09:59:16.768217Z","iopub.execute_input":"2024-11-24T09:59:16.768629Z","iopub.status.idle":"2024-11-24T10:21:25.686561Z","shell.execute_reply.started":"2024-11-24T09:59:16.768594Z","shell.execute_reply":"2024-11-24T10:21:25.685181Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating model: RandomForest\nTraining Classification Report for RandomForestClassifier:\n              precision    recall  f1-score   support\n\n           0       0.68      0.77      0.72   1873613\n           1       0.79      0.70      0.75   2341288\n\n    accuracy                           0.73   4214901\n   macro avg       0.73      0.74      0.73   4214901\nweighted avg       0.74      0.73      0.73   4214901\n\nValidation Classification Report for RandomForestClassifier:\n              precision    recall  f1-score   support\n\n           0       0.64      0.41      0.50    712298\n           1       0.60      0.79      0.68    781776\n\n    accuracy                           0.61   1494074\n   macro avg       0.62      0.60      0.59   1494074\nweighted avg       0.62      0.61      0.59   1494074\n\n\nEvaluating model: LightGBM\n[LightGBM] [Info] Number of positive: 2341288, number of negative: 1873613\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.047973 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 4214901, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\nTraining Classification Report for LGBMClassifier:\n              precision    recall  f1-score   support\n\n           0       0.56      0.65      0.60   1873613\n           1       0.68      0.59      0.63   2341288\n\n    accuracy                           0.62   4214901\n   macro avg       0.62      0.62      0.62   4214901\nweighted avg       0.62      0.62      0.62   4214901\n\nValidation Classification Report for LGBMClassifier:\n              precision    recall  f1-score   support\n\n           0       0.62      0.45      0.52    712298\n           1       0.60      0.74      0.66    781776\n\n    accuracy                           0.60   1494074\n   macro avg       0.61      0.60      0.59   1494074\nweighted avg       0.61      0.60      0.59   1494074\n\n\nEvaluating model: XGBoost\nTraining Classification Report for XGBClassifier:\n              precision    recall  f1-score   support\n\n           0       0.56      0.66      0.60   1873613\n           1       0.68      0.59      0.63   2341288\n\n    accuracy                           0.62   4214901\n   macro avg       0.62      0.62      0.62   4214901\nweighted avg       0.63      0.62      0.62   4214901\n\nValidation Classification Report for XGBClassifier:\n              precision    recall  f1-score   support\n\n           0       0.61      0.46      0.52    712298\n           1       0.60      0.74      0.66    781776\n\n    accuracy                           0.60   1494074\n   macro avg       0.61      0.60      0.59   1494074\nweighted avg       0.61      0.60      0.60   1494074\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nimport lightgbm as lgb\n\n\nparam_dist = {\n    'classifier__num_leaves': [31, 50, 100],  # Количество листьев в дереве\n    'classifier__max_depth': [-1, 5, 10, 20],  # Максимальная глубина деревьев\n    'classifier__learning_rate': [0.01, 0.05, 0.1, 0.2],  # Скорость обучения\n    'classifier__n_estimators': [50, 100, 200],  # Количество деревьев\n    'classifier__subsample': [0.6, 0.8, 1.0],  # Часть данных для каждого дерева\n    'classifier__colsample_bytree': [0.6, 0.8, 1.0],  # Часть признаков для каждого дерева\n}\n\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),  \n    ('classifier', lgb.LGBMClassifier(random_state=42, class_weight='balanced'))\n])\n\n\nrandom_search = RandomizedSearchCV(\n    pipeline, \n    param_distributions=param_dist, \n    n_iter=20,                # Количество итераций поиска\n    scoring='f1_weighted',    \n    cv=3,                     \n    random_state=42,          \n    n_jobs=-1,                \n    verbose=2                 \n)\n\n\nrandom_search.fit(X_train, y_train)\n\n\nbest_model = random_search.best_estimator_\n\n\nprint(\"Лучшие параметры:\", random_search.best_params_)\n\n\ny_train_pred = best_model.predict(X_train)\nprint(\"Training Classification Report:\")\nprint(classification_report(y_train, y_train_pred))\n\ny_val_pred = best_model.predict(X_val)\nprint(\"Validation Classification Report:\")\nprint(classification_report(y_val, y_val_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T10:37:30.219316Z","iopub.execute_input":"2024-11-24T10:37:30.221078Z","iopub.status.idle":"2024-11-24T11:16:27.654810Z","shell.execute_reply.started":"2024-11-24T10:37:30.221024Z","shell.execute_reply":"2024-11-24T11:16:27.653759Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 20 candidates, totalling 60 fits\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041115 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=50, classifier__num_leaves=50, classifier__subsample=0.8; total time=  17.7s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041350 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=50, classifier__num_leaves=50, classifier__subsample=0.8; total time=  20.0s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040993 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=50, classifier__num_leaves=50, classifier__subsample=0.8; total time=  19.8s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030824 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.2, classifier__max_depth=-1, classifier__n_estimators=200, classifier__num_leaves=50, classifier__subsample=0.8; total time=  45.0s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031265 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.2, classifier__max_depth=-1, classifier__n_estimators=200, classifier__num_leaves=50, classifier__subsample=0.8; total time=  46.8s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031303 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.2, classifier__max_depth=-1, classifier__n_estimators=200, classifier__num_leaves=50, classifier__subsample=0.8; total time=  47.8s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040710 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=100, classifier__num_leaves=100, classifier__subsample=0.8; total time=  28.8s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040607 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=100, classifier__num_leaves=100, classifier__subsample=0.8; total time=  34.8s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041308 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=100, classifier__num_leaves=100, classifier__subsample=0.8; total time=  34.3s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049435 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=50, classifier__num_leaves=50, classifier__subsample=0.6; total time=  17.1s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049579 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=50, classifier__num_leaves=50, classifier__subsample=0.6; total time=  20.9s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051550 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.05, classifier__max_depth=10, classifier__n_estimators=50, classifier__num_leaves=50, classifier__subsample=0.6; total time=  21.0s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049989 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=0.6; total time=  46.3s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050091 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=0.6; total time=  55.8s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.054549 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=0.6; total time=  53.8s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049617 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.05, classifier__max_depth=-1, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=1.0; total time=  29.5s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049715 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.05, classifier__max_depth=-1, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=1.0; total time=  36.4s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050008 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.05, classifier__max_depth=-1, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=1.0; total time=  37.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040471 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=200, classifier__num_leaves=31, classifier__subsample=0.8; total time=  50.5s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040498 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=200, classifier__num_leaves=31, classifier__subsample=0.8; total time=  54.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040916 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=200, classifier__num_leaves=31, classifier__subsample=0.8; total time=  52.4s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040956 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.05, classifier__max_depth=20, classifier__n_estimators=50, classifier__num_leaves=100, classifier__subsample=0.8; total time=  19.8s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.046033 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.05, classifier__max_depth=20, classifier__n_estimators=50, classifier__num_leaves=100, classifier__subsample=0.8; total time=  22.0s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041023 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.05, classifier__max_depth=20, classifier__n_estimators=50, classifier__num_leaves=100, classifier__subsample=0.8; total time=  22.6s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050307 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=-1, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=1.0; total time= 1.1min\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049839 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=-1, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=1.0; total time= 1.3min\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050772 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=-1, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=1.0; total time= 1.3min\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030792 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__num_leaves=50, classifier__subsample=0.8; total time=  54.4s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030894 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__num_leaves=50, classifier__subsample=0.8; total time=  57.7s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031356 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=200, classifier__num_leaves=50, classifier__subsample=0.8; total time=  58.0s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040860 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=50, classifier__num_leaves=100, classifier__subsample=0.8; total time=  19.4s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040971 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=50, classifier__num_leaves=100, classifier__subsample=0.8; total time=  22.3s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040843 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=50, classifier__num_leaves=100, classifier__subsample=0.8; total time=  22.4s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.049771 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=0.6; total time=  45.1s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050466 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=0.6; total time=  47.8s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050279 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.1, classifier__max_depth=5, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=0.6; total time=  46.8s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040844 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__num_leaves=50, classifier__subsample=0.8; total time=  44.4s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045521 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__num_leaves=50, classifier__subsample=0.8; total time=  52.3s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041352 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=10, classifier__n_estimators=200, classifier__num_leaves=50, classifier__subsample=0.8; total time=  52.6s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040868 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=50, classifier__num_leaves=50, classifier__subsample=0.6; total time=  14.9s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041118 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=50, classifier__num_leaves=50, classifier__subsample=0.6; total time=  15.1s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041350 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=50, classifier__num_leaves=50, classifier__subsample=0.6; total time=  16.0s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050012 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__num_leaves=31, classifier__subsample=0.8; total time=  53.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050264 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__num_leaves=31, classifier__subsample=0.8; total time= 1.0min\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.050672 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=1.0, classifier__learning_rate=0.01, classifier__max_depth=20, classifier__n_estimators=200, classifier__num_leaves=31, classifier__subsample=0.8; total time= 1.1min\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030733 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100, classifier__num_leaves=100, classifier__subsample=1.0; total time=  24.9s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031215 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100, classifier__num_leaves=100, classifier__subsample=1.0; total time=  25.2s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031178 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.01, classifier__max_depth=5, classifier__n_estimators=100, classifier__num_leaves=100, classifier__subsample=1.0; total time=  25.8s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031049 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__num_leaves=31, classifier__subsample=1.0; total time=  25.3s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031107 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__num_leaves=31, classifier__subsample=1.0; total time=  26.9s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.034503 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.01, classifier__max_depth=10, classifier__n_estimators=100, classifier__num_leaves=31, classifier__subsample=1.0; total time=  27.2s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040837 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=50, classifier__num_leaves=31, classifier__subsample=1.0; total time=  15.6s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040590 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=50, classifier__num_leaves=31, classifier__subsample=1.0; total time=  17.4s\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.186536 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=20, classifier__n_estimators=50, classifier__num_leaves=31, classifier__subsample=1.0; total time=  19.4s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040558 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=-1, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=0.8; total time=  46.0s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.040785 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=-1, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=0.8; total time=  54.3s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.041257 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[CV] END classifier__colsample_bytree=0.8, classifier__learning_rate=0.1, classifier__max_depth=-1, classifier__n_estimators=200, classifier__num_leaves=100, classifier__subsample=0.8; total time=  54.2s\n[LightGBM] [Info] Number of positive: 1560858, number of negative: 1249076\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.030989 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 934\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.2, classifier__max_depth=5, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=0.6; total time=  24.8s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031253 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 950\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.2, classifier__max_depth=5, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=0.6; total time=  24.2s\n[LightGBM] [Info] Number of positive: 1560859, number of negative: 1249075\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.031687 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 2809934, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n[LightGBM] [Info] Start training from score 0.000000\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[CV] END classifier__colsample_bytree=0.6, classifier__learning_rate=0.2, classifier__max_depth=5, classifier__n_estimators=100, classifier__num_leaves=50, classifier__subsample=0.6; total time=  24.4s\n[LightGBM] [Info] Number of positive: 2341288, number of negative: 1873613\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.075485 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 956\n[LightGBM] [Info] Number of data points in the train set: 4214901, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n[LightGBM] [Info] Start training from score -0.000000\nЛучшие параметры: {'classifier__subsample': 1.0, 'classifier__num_leaves': 50, 'classifier__n_estimators': 100, 'classifier__max_depth': -1, 'classifier__learning_rate': 0.05, 'classifier__colsample_bytree': 1.0}\nTraining Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.56      0.65      0.60   1873613\n           1       0.68      0.59      0.63   2341288\n\n    accuracy                           0.62   4214901\n   macro avg       0.62      0.62      0.62   4214901\nweighted avg       0.62      0.62      0.62   4214901\n\nValidation Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.62      0.45      0.52    712298\n           1       0.60      0.75      0.66    781776\n\n    accuracy                           0.60   1494074\n   macro avg       0.61      0.60      0.59   1494074\nweighted avg       0.61      0.60      0.59   1494074\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:34:22.674012Z","iopub.execute_input":"2024-11-24T11:34:22.674430Z","iopub.status.idle":"2024-11-24T11:34:22.689940Z","shell.execute_reply.started":"2024-11-24T11:34:22.674397Z","shell.execute_reply":"2024-11-24T11:34:22.688669Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"      id  target  user_id cart  avg_order_size  unique_category_count  \\\n0  0;133       0        0  133             0.0                    0.0   \n1    0;5       1        0    5             0.0                    0.0   \n2   0;10       0        0   10             0.0                    0.0   \n3  0;396       1        0  396             0.0                    0.0   \n4   0;14       0        0   14             0.0                    0.0   \n\n  last_order_date  \n0      1900-01-01  \n1      1900-01-01  \n2      1900-01-01  \n3      1900-01-01  \n4      1900-01-01  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n      <th>user_id</th>\n      <th>cart</th>\n      <th>avg_order_size</th>\n      <th>unique_category_count</th>\n      <th>last_order_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0;133</td>\n      <td>0</td>\n      <td>0</td>\n      <td>133</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1900-01-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0;5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1900-01-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0;10</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1900-01-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0;396</td>\n      <td>1</td>\n      <td>0</td>\n      <td>396</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1900-01-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0;14</td>\n      <td>0</td>\n      <td>0</td>\n      <td>14</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1900-01-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# ids = test['id']\n# test = test.drop(columns=['id', 'target'])\n\n# X_test = preprocessor.transform(test)\ny_test_pred = best_model.predict(test)\n\nsubmission_df = pd.DataFrame({\n    'id': ids,\n    'target': y_test_pred\n})\n\nprint(submission_df.head())\n# print(test.head())\n# print(X_val.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:43:43.922092Z","iopub.execute_input":"2024-11-24T11:43:43.922512Z","iopub.status.idle":"2024-11-24T11:43:47.282639Z","shell.execute_reply.started":"2024-11-24T11:43:43.922476Z","shell.execute_reply":"2024-11-24T11:43:47.281381Z"}},"outputs":[{"name":"stdout","text":"      id  target\n0  0;133       1\n1    0;5       1\n2   0;10       1\n3  0;396       1\n4   0;14       1\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:43:50.733588Z","iopub.execute_input":"2024-11-24T11:43:50.735051Z","iopub.status.idle":"2024-11-24T11:43:51.387587Z","shell.execute_reply.started":"2024-11-24T11:43:50.734998Z","shell.execute_reply":"2024-11-24T11:43:51.386374Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"from shutil import move\nmove('submission.csv', '/kaggle/working/submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-24T11:46:36.573527Z","iopub.execute_input":"2024-11-24T11:46:36.574037Z","iopub.status.idle":"2024-11-24T11:46:36.581621Z","shell.execute_reply.started":"2024-11-24T11:46:36.573997Z","shell.execute_reply":"2024-11-24T11:46:36.580402Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/submission.csv'"},"metadata":{}}],"execution_count":37}]}